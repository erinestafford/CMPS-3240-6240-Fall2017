{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description: \n",
    "This data is about housing includes the information about the size of the house, number of bedrooms, the location and a few other features. The goal is to predict the price of the house based on these features. \n",
    "\n",
    "Using 30/70 test/train split(30% test and 70% train):\n",
    "1) Implement a gradient descent algorithm that can do regression and predicts the price. \n",
    "Report the mean squared error for train and test. \n",
    "2) Choose a threshold T and assume all the houses with a price larger than T are labeled as expensive and the rest are labeled as non-expensive. Implement a simple perceptron that can classify the houses as expensive/non-expensive for you. Report accuracy for train and test.\n",
    "You are free to use any language, but the algorithms should be implemented by yourself. Data is attached.\n",
    "\n",
    "Please make a folder in your Github repository for this assignment and commit your code there. Then post the link to the repository + results, in canvas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Implement a gradient descent algorithm that can do regression and predicts the price. Report the mean squared error for train and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#computes output vector by finding the dot product of each row of features by the weights\n",
    "def compute_output(w,x):\n",
    "    output = np.dot(x,w.T)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#computes squarred error by summing the error and dividing by 2\n",
    "def get_squared_error(error):\n",
    "    error = np.square(error)\n",
    "    sum_error = error.sum()\n",
    "    sum_error = sum_error/2\n",
    "    return sum_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#implement gradient descent\n",
    "import numpy as np\n",
    "def findModel(x,y):\n",
    "    w = np.random.rand(5) #initial weights\n",
    "    w = np.matrix(w)\n",
    "    R = .001 # learning rate\n",
    "    output = compute_output(w,x) #initial price predictions\n",
    "    iterations = 0 #iteration count\n",
    "    max_it = 10000000 #maximum number of iterations\n",
    "    squared_error = 10000000 # arbitrary initial squared error\n",
    "    tol = .01\n",
    "    prev_errors = []\n",
    "    while((squared_error > tol) and (iterations < max_it)): #gradient descent\n",
    "        errors= np.array(output) - np.array(y)\n",
    "        squared_error = get_squared_error(errors)#get squared error\n",
    "        prev_errors.append(squared_error)\n",
    "        # dE/dwi = sum(error[j]*-x[i][j]), where j is example, and i is features column\n",
    "        gradient_E = x.T.dot(errors) #should be a 5x1 vector of dE/dwi\n",
    "        #update weights\n",
    "        w = w.T - (R* gradient_E)\n",
    "        w = w.T\n",
    "        iterations += 1#increase iteration count\n",
    "        output = compute_output(w,x) #compute output with new weights\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.990413779238\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "#read in features\n",
    "f = open(\"housing_prices.txt\")\n",
    "lines = f.readlines()\n",
    "lines = lines[1:]\n",
    "features = [[]]*(len(lines))\n",
    "count = 0\n",
    "for line in lines:\n",
    "    features[count] = line.split()\n",
    "    tempStreet = \"\"\n",
    "    stringcount = 0\n",
    "    for f in features[count]:\n",
    "        try:\n",
    "            f = float(f)\n",
    "        except ValueError:\n",
    "            tempStreet += f + \" \"\n",
    "            stringcount +=1\n",
    "    features[count].append(tempStreet)\n",
    "    for i in range(stringcount):\n",
    "        features[count].pop(2)\n",
    "    count += 1\n",
    "y = [] #output\n",
    "x = [] #input\n",
    "for f in features:\n",
    "    y.append(float(f[1]))\n",
    "    f.pop(1)\n",
    "    f[0]= float(f[0])\n",
    "    f[1]= float(f[1])\n",
    "    f[2]= float(f[2])\n",
    "    x.append(f)\n",
    "        \n",
    "#make dictionary to map street names to values\n",
    "#replace street names with values\n",
    "street_names = {}\n",
    "count3 = 1\n",
    "for i in range(len(x)):\n",
    "    if x[i][3] not in street_names.keys():\n",
    "        street_names[x[i][3]] = count3\n",
    "        count3 += 1\n",
    "    x[i][3] = street_names[x[i][3]]\n",
    "    x[i] = [1] + x[i]#create x0 column\n",
    "#rescale features\n",
    "#using x' = (x - min)/(max-min)\n",
    "x1 = []\n",
    "x2 = []\n",
    "x3 = []\n",
    "x4 = []\n",
    "for i in range(len(x)):\n",
    "    x1.append(x[i][1])\n",
    "    x2.append(x[i][2])\n",
    "    x3.append(x[i][3])\n",
    "    x4.append(x[i][4])\n",
    "#store pre-normalized values\n",
    "y_real = []\n",
    "ymax = max(y)\n",
    "ymin = min(y)\n",
    "#normalize\n",
    "for i in range(len(x)):\n",
    "    x[i][1] = (x[i][1] - min(x1))/(max(x1)-min(x1))\n",
    "    x[i][2] = (x[i][2] - min(x2))/(max(x2)-min(x2))\n",
    "    x[i][3] = (x[i][3] - min(x3))/(max(x3)-min(x3))\n",
    "    x[i][4] = (x[i][4] - min(x4))/(max(x4)-min(x4))\n",
    "    y_real.append(y[i])\n",
    "    y[i] = (y[i] - min(y))/(max(y) - min(y))\n",
    "#divide train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.30)\n",
    "x_train = np.matrix(x_train)\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "x_test = np.matrix(x_test)\n",
    "y_test = np.matrix(y_test)\n",
    "#w = findModel(x_train,y_train)\n",
    "#print(w)\n",
    "w = [ 0.08793469,  0.71704488, -0.08180255,  0.00179489,  0.12905482]\n",
    "w = np.matrix(w)\n",
    "o = compute_output(w,x_train)\n",
    "errors= o - y_train\n",
    "se = get_squared_error(errors)\n",
    "print(se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For normalized data: \n",
    "since the weight vector is initialized to be random, the output of the function is different each time it's run\n",
    "\n",
    " w= [ 0.08793469,  0.71704488, -0.08180255,  0.00179489,  0.12905482]\n",
    " \n",
    " b = 0.08793469\n",
    " \n",
    " w1 = 0.71704488\n",
    " \n",
    " w2 = -0.08180255\n",
    " \n",
    " w3 = 0.00179489\n",
    " \n",
    " w4 = 0.12905482\n",
    " \n",
    " Train Error = 0.990413779238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.529929911934\n"
     ]
    }
   ],
   "source": [
    "o_test = compute_output(w, x_test)\n",
    "y_test = y_test.T\n",
    "errors_test = o_test - y_test\n",
    "se_test = get_squared_error(errors_test)\n",
    "print(se_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For normalized data:\n",
    "Test Error = 0.529929911934\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2) Choose a threshold T and assume all the houses with a price larger than T are labeled as expensive and the rest are labeled as non-expensive. Implement a simple perceptron that can classify the houses as expensive/non-expensive for you. Report accuracy for train and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create classifications vector\n",
    "T = 500000\n",
    "house_classification = []\n",
    "for p in y_real:\n",
    "    if p >= 500000:\n",
    "        house_classification.append(1)\n",
    "    else:\n",
    "        house_classification.append(-1)\n",
    "#train/test classifications\n",
    "x_train, x_test, y_train, y_test = train_test_split( x, house_classification, test_size=0.30)\n",
    "x_train = np.matrix(x_train)\n",
    "y_train = np.matrix(y_train)\n",
    "y_train = y_train.T\n",
    "x_test = np.matrix(x_test)\n",
    "y_test = np.matrix(y_test)\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(w,x):\n",
    "    classifications = []\n",
    "    output = np.dot(x,w.T)\n",
    "    for o in output:\n",
    "        if o >= 0:\n",
    "            classifications.append(1)\n",
    "        else:\n",
    "            classifications.append(-1)\n",
    "    return np.matrix(classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findModelClassify(x,y):\n",
    "    w = np.random.rand(5) #initial weights\n",
    "    w = np.matrix(w)\n",
    "    R = .0005 # learning rate\n",
    "    output = classify(w,x) #initial price predictions\n",
    "    iterations = 0 #iteration count\n",
    "    max_it = 100000 #maximum number of iterations\n",
    "    squared_error = 10000000 # arbitrary initial squared error\n",
    "    tol = .01\n",
    "    prev_errors = []\n",
    "    while((squared_error > tol) and (iterations < max_it)): #gradient descent\n",
    "        errors= np.array(output.T) - np.array(y)\n",
    "        squared_error = get_squared_error(errors)#get squared error\n",
    "        #print(squared_error)\n",
    "        prev_errors.append(squared_error)\n",
    "        # dE/dwi = sum(error[j]*-x[i][j]), where j is example, and i is features column\n",
    "        gradient_E = np.dot(x.T,errors) #should be a 5x1 vector of dE/dwi\n",
    "        #update weights\n",
    "        w = w.T - (R* gradient_E)\n",
    "        w = w.T\n",
    "        iterations += 1#increase iteration count\n",
    "        output = classify(w,x) #compute output with new weights\n",
    "    print(squared_error)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.0\n",
      "[[-0.01559445  0.08650073 -0.03168312 -0.0234954  -0.00222042]]\n"
     ]
    }
   ],
   "source": [
    "w = findModelClassify(x_train,y_train)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "o_test = classify(w,x_test)\n",
    "test_errors= np.array(o_test.T) - np.array(y_test)\n",
    "se = get_squared_error(test_errors)\n",
    "print(se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Error\n",
    "\n",
    "For train: \n",
    "\n",
    "Squared Error = 26.0\n",
    "\n",
    "For test:\n",
    "\n",
    "Squared Error = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
